{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - This Notebook is Created by **Mohd Muttalib**\n",
    "\n",
    "- **Email** - muttalib1326@gmail.com\n",
    "\n",
    "\n",
    "\n",
    "- **Contact** - +918445818187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score,classification_report,confusion_matrix,log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X49</th>\n",
       "      <th>X50</th>\n",
       "      <th>X51</th>\n",
       "      <th>X52</th>\n",
       "      <th>X53</th>\n",
       "      <th>X54</th>\n",
       "      <th>X55</th>\n",
       "      <th>X56</th>\n",
       "      <th>X57</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.342</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.200</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.083</td>\n",
       "      <td>16.304</td>\n",
       "      <td>148</td>\n",
       "      <td>375</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.440</td>\n",
       "      <td>22</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>35.46</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.338</td>\n",
       "      <td>123</td>\n",
       "      <td>207</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     X1    X2    X3     X4    X5    X6    X7    X8   X9   X10  ...  X49  \\\n",
       "0  0.00  0.00  4.34   0.00  0.00  0.00  0.00  0.00  0.0  0.00  ...  0.0   \n",
       "1  0.00  0.56  0.56   0.00  1.12  0.56  2.25  0.00  0.0  0.56  ...  0.0   \n",
       "2  0.00  0.00  0.00   0.00  0.00  0.00  0.00  0.00  0.0  0.00  ...  0.0   \n",
       "3  0.64  0.00  0.64   0.00  1.93  0.00  0.00  0.00  0.0  0.00  ...  0.0   \n",
       "4  0.58  0.00  0.00  35.46  0.58  0.00  0.58  0.58  0.0  0.00  ...  0.0   \n",
       "\n",
       "     X50  X51    X52    X53    X54     X55  X56  X57  Y  \n",
       "0  0.000  0.0  1.342  0.000  0.000   1.200    2   12  0  \n",
       "1  0.083  0.0  0.503  0.000  0.083  16.304  148  375  1  \n",
       "2  0.000  0.0  0.000  0.000  0.000   1.000    1    5  0  \n",
       "3  0.000  0.0  0.462  0.370  0.000   2.440   22  122  1  \n",
       "4  0.000  0.0  0.239  0.239  0.000   3.338  123  207  1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the train data\n",
    "df = pd.read_csv('training_set.csv',index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X48</th>\n",
       "      <th>X49</th>\n",
       "      <th>X50</th>\n",
       "      <th>X51</th>\n",
       "      <th>X52</th>\n",
       "      <th>X53</th>\n",
       "      <th>X54</th>\n",
       "      <th>X55</th>\n",
       "      <th>X56</th>\n",
       "      <th>X57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.342</td>\n",
       "      <td>47</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.375</td>\n",
       "      <td>168</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.74</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.153</td>\n",
       "      <td>5.891</td>\n",
       "      <td>193</td>\n",
       "      <td>3040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.015</td>\n",
       "      <td>8.550</td>\n",
       "      <td>669</td>\n",
       "      <td>1351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.781</td>\n",
       "      <td>32</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     X1   X2    X3   X4    X5    X6    X7    X8    X9   X10  ...  X48  X49  \\\n",
       "0  0.70  0.0  0.70  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.0  0.0   \n",
       "1  0.00  0.0  0.84  0.0  0.84  0.00  0.84  0.00  0.00  0.00  ...  0.0  0.0   \n",
       "2  0.46  0.3  0.46  0.0  0.05  0.12  0.05  0.28  0.43  0.74  ...  0.0  0.0   \n",
       "3  0.10  0.2  1.01  0.0  0.80  0.80  0.50  0.00  0.80  0.10  ...  0.0  0.0   \n",
       "4  0.00  0.0  0.72  0.0  0.72  0.00  0.72  0.00  0.00  0.00  ...  0.0  0.0   \n",
       "\n",
       "     X50  X51    X52    X53    X54     X55  X56   X57  \n",
       "0  0.000  0.0  0.105  0.000  0.000   2.342   47    89  \n",
       "1  0.388  0.0  0.776  0.129  0.000  10.375  168   249  \n",
       "2  0.065  0.0  0.325  0.756  0.153   5.891  193  3040  \n",
       "3  0.110  0.0  0.490  0.158  0.015   8.550  669  1351  \n",
       "4  0.364  0.0  0.729  0.121  0.000   7.781   32   249  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Test data\n",
    "test_data = pd.read_csv('test_set.csv',index_col=0)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Y'],axis=1)\n",
    "y = df['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RandomForest Classifier for feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(100, max_depth=None, n_jobs=-1)\n",
    "clf.fit(X_train,y_train)\n",
    "feature_importance = clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking the features with their respect to feature importances\n",
    "fi = sorted(zip(X.columns,feature_importance),key=lambda x: x[1], reverse=True)\n",
    "# Extracting Top 30 features\n",
    "top_features = [x[0] for x in fi[:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the top features from data\n",
    "X_train_dash = X_train[top_features]\n",
    "X_test_dash = X_test[top_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the top features for test data\n",
    "test_data_dash = test_data[top_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using StandardScaler to normalize our data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_dash)\n",
    "\n",
    "# transform\n",
    "X_train_dash = pd.DataFrame(scaler.transform(X_train_dash),columns=X_train_dash.columns)\n",
    "X_test_dash = pd.DataFrame(scaler.transform(X_test_dash),columns=X_test_dash.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using StandardScaler to normalize our test data\n",
    "test_data_dash = pd.DataFrame(scaler.transform(test_data_dash),columns=test_data_dash.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Logloss for the model -> 0.14337571574014177\n",
      "Validation Logloss for the model -> 0.15469231195819308\n",
      "--------------------------------------------------\n",
      "Train AUC Score for the model -> 0.9890845850873322\n",
      "Validation AUC Score for the model -> 0.9881158923367048\n"
     ]
    }
   ],
   "source": [
    "# Using Xgboost\n",
    "classifier = XGBClassifier(n_estimators=500,\n",
    "                           max_depth=5,\n",
    "                           learning_rate=0.15,\n",
    "                           colsample_bytree=1,\n",
    "                           subsample=1,\n",
    "                           reg_alpha = 0.3,\n",
    "                           gamma=10,\n",
    "                           n_jobs=-1,\n",
    "                           eval_metric='logloss',\n",
    "                           use_label_encoder=False)\n",
    "\n",
    "classifier.fit(X_train_dash, y_train)\n",
    "\n",
    "y_train_pred = classifier.predict(X_train_dash)\n",
    "y_train_prob = classifier.predict_proba(X_train_dash)[:,1]\n",
    "y_val_pred = classifier.predict(X_test_dash)\n",
    "y_val_prob = classifier.predict_proba(X_test_dash)[:,1]\n",
    "\n",
    "\n",
    "# Calculating logloss score for our model\n",
    "print(f'Train Logloss for the model -> {log_loss(y_train,y_train_prob)}')\n",
    "print(f'Validation Logloss for the model -> {log_loss(y_val,y_val_prob)}')\n",
    "\n",
    "print('-'*50)\n",
    "# Calculating the AUC score for our model\n",
    "print(f'Train AUC Score for the model -> {roc_auc_score(y_train, y_train_prob)}')\n",
    "print(f'Validation AUC Score for the model -> {roc_auc_score(y_val, y_val_prob)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1\n",
      " 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 0 1 0\n",
      " 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0\n",
      " 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0\n",
      " 0 1 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1\n",
      " 1 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0\n",
      " 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1\n",
      " 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1\n",
      " 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 0\n",
      " 1 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0\n",
      " 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1\n",
      " 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0\n",
      " 1 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1\n",
      " 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = classifier.predict(test_data_dash)\n",
    "print(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04236659 0.9799453  0.9817722  0.973751   0.978819   0.20681825\n",
      " 0.9771714  0.7067784  0.9675239  0.983721   0.01044433 0.07185189\n",
      " 0.21114746 0.01652415 0.05903362 0.11397144 0.02350036 0.16215698\n",
      " 0.9698316  0.02088377 0.9759626  0.97873086 0.9744621  0.01000602\n",
      " 0.77090746 0.06609312 0.1032019  0.84450877 0.06609312 0.01105931\n",
      " 0.89442337 0.05260878 0.01652415 0.05201242 0.01225299 0.03658777\n",
      " 0.4534763  0.02922887 0.5368635  0.46999773 0.9838521  0.09450296\n",
      " 0.16215698 0.12399059 0.04577107 0.9401931  0.07101598 0.01805492\n",
      " 0.03803167 0.9736239  0.85039765 0.10282127 0.01045498 0.9307046\n",
      " 0.9787997  0.97803295 0.01926898 0.01777293 0.45117712 0.95659757\n",
      " 0.97755414 0.163437   0.3375191  0.2114131  0.35615364 0.965411\n",
      " 0.9304606  0.7690301  0.11353943 0.05201242 0.02389444 0.8761709\n",
      " 0.05201242 0.5587642  0.9270567  0.97100306 0.0962676  0.01069072\n",
      " 0.31966805 0.05201242 0.08003447 0.94516116 0.944969   0.9700033\n",
      " 0.05201242 0.01652415 0.09344687 0.05201242 0.96715164 0.0128664\n",
      " 0.05973293 0.12119357 0.02390428 0.08891153 0.04666426 0.80579275\n",
      " 0.6400324  0.09590127 0.97939336 0.7716517  0.01044433 0.02673295\n",
      " 0.23485747 0.7241554  0.9762437  0.04167371 0.22797766 0.04584917\n",
      " 0.03653617 0.9603509  0.09424327 0.8971042  0.03671601 0.9724992\n",
      " 0.98117596 0.04747988 0.22797766 0.05973293 0.02929307 0.03119038\n",
      " 0.23362866 0.03208727 0.04503499 0.18303646 0.09103058 0.02861903\n",
      " 0.04967488 0.5874416  0.01652415 0.9625242  0.5680753  0.07722913\n",
      " 0.06086551 0.05583202 0.9706943  0.05201242 0.07602189 0.5768118\n",
      " 0.0760066  0.98147774 0.06609312 0.04005877 0.07319447 0.02353823\n",
      " 0.0128664  0.94860643 0.94325835 0.21074401 0.76949656 0.9566469\n",
      " 0.01105816 0.9695281  0.9775375  0.03481971 0.05044426 0.03035192\n",
      " 0.9735286  0.05201242 0.00832988 0.4436344  0.97958905 0.063737\n",
      " 0.01652415 0.02432353 0.8351055  0.03116163 0.05841103 0.05973293\n",
      " 0.1127404  0.0115298  0.03602865 0.9835527  0.23350622 0.9788983\n",
      " 0.02569744 0.10718615 0.9834411  0.05201242 0.9713192  0.9361454\n",
      " 0.02678061 0.03752691 0.0122628  0.973751   0.12831016 0.11059147\n",
      " 0.9467814  0.30713505 0.0761686  0.98298085 0.81213975 0.05217764\n",
      " 0.973751   0.6826058  0.8057185  0.8473379  0.05500587 0.05260878\n",
      " 0.24488762 0.97003037 0.05201242 0.9508579  0.08387985 0.00978671\n",
      " 0.08043128 0.9568525  0.8799715  0.03653617 0.981605   0.07729732\n",
      " 0.36429417 0.96045434 0.02682479 0.01763647 0.03646734 0.10637331\n",
      " 0.08055083 0.10718615 0.0128664  0.9808791  0.01898821 0.9719438\n",
      " 0.02552149 0.1946253  0.05973293 0.8227276  0.96230745 0.05201242\n",
      " 0.05201242 0.00832988 0.11253413 0.03720487 0.9089911  0.01082723\n",
      " 0.05973293 0.24449888 0.9763674  0.03571861 0.05073442 0.00897339\n",
      " 0.03686791 0.06209239 0.03561629 0.01744287 0.14620477 0.09632216\n",
      " 0.09173968 0.02284816 0.05260643 0.9768524  0.06609312 0.9520975\n",
      " 0.05451051 0.8437396  0.01920235 0.03646734 0.57555985 0.5829445\n",
      " 0.9765712  0.9563398  0.02388351 0.01652415 0.95028675 0.05201242\n",
      " 0.9698316  0.03163756 0.05201242 0.16215698 0.6703656  0.03925145\n",
      " 0.01652415 0.9762991  0.9817722  0.06265064 0.9776187  0.6892549\n",
      " 0.57853836 0.06279906 0.01751286 0.01044433 0.01115933 0.01652415\n",
      " 0.8819383  0.15380916 0.9069848  0.11997376 0.05201242 0.9309659\n",
      " 0.10280185 0.06970519 0.5735243  0.03009395 0.0362644  0.85039765\n",
      " 0.9066571  0.00832988 0.43764073 0.03387704 0.05201242 0.96988195\n",
      " 0.02037857 0.9693653  0.00832988 0.02954213 0.25884205 0.02852306\n",
      " 0.01877704 0.9786563  0.01842143 0.96218896 0.7025471  0.9611117\n",
      " 0.05148628 0.04056003 0.8423878  0.04081686 0.02068215 0.08316173\n",
      " 0.03236111 0.02565753 0.07974911 0.16215698 0.13336283 0.98041487\n",
      " 0.01652415 0.91027784 0.02415852 0.973751   0.03614154 0.01652415\n",
      " 0.02569744 0.7740222  0.75966513 0.05973293 0.5783537  0.70959264\n",
      " 0.06021285 0.2347952  0.07033565 0.06819054 0.23485747 0.02244115\n",
      " 0.05201242 0.9725839  0.04355509 0.05973293 0.6802234  0.02258355\n",
      " 0.15518406 0.35276425 0.0362644  0.9765712  0.06813176 0.01321637\n",
      " 0.97013277 0.02258355 0.02258355 0.06819054 0.52113783 0.38482597\n",
      " 0.05201242 0.03201954 0.06299906 0.0584032  0.052687   0.03010747\n",
      " 0.9669299  0.0197968  0.61341935 0.94476384 0.89723474 0.93791044\n",
      " 0.10017007 0.00978671 0.7101262  0.03062816 0.08438793 0.93760777\n",
      " 0.98544794 0.96029896 0.97803295 0.9803357  0.03106942 0.0676239\n",
      " 0.01652415 0.02461231 0.9791589  0.05201242 0.02468498 0.9808791\n",
      " 0.869892   0.98573303 0.39589447 0.97652656 0.9777828  0.97993004\n",
      " 0.04150391 0.07644925 0.01652415 0.9505078  0.9071592  0.9780862\n",
      " 0.9584738  0.9460618  0.97206974 0.9381757  0.08768834 0.9732921\n",
      " 0.05673723 0.02700157 0.02350036 0.98339844 0.97799736 0.04010939\n",
      " 0.02945317 0.04416974 0.02028068 0.03311786 0.05201242 0.96783966\n",
      " 0.9699495  0.8844951  0.06813176 0.23485747 0.9494889  0.11801768\n",
      " 0.03272953 0.12204207 0.14483035 0.11619923 0.9584738  0.2714998\n",
      " 0.02303786 0.06921297 0.07224997 0.03937611 0.06085661 0.977648\n",
      " 0.9834821  0.22673537 0.2152836  0.97307384 0.9708035  0.23396549\n",
      " 0.03614154 0.07804797 0.01570011 0.9838521  0.9545466  0.09470761\n",
      " 0.6200511  0.93906313 0.9643413  0.0270295  0.0742801  0.9615417\n",
      " 0.09800918 0.08732539 0.96067476 0.95389044 0.01044433 0.06609312\n",
      " 0.9397929  0.8227276  0.0197968  0.9053642  0.09695508 0.01652415\n",
      " 0.9638386  0.9304363  0.00910274 0.28941745 0.9767493  0.04577107\n",
      " 0.2943303  0.9423009  0.9821186  0.93972725 0.97903425 0.91000843\n",
      " 0.01652415 0.05627444 0.98251295 0.01833717 0.0653441  0.2270118\n",
      " 0.01620663 0.11236934 0.0197968  0.05201242 0.02450003 0.95047736\n",
      " 0.04056003 0.2593983  0.912137   0.96302915 0.92924297 0.46205303\n",
      " 0.9478438  0.09326253 0.44520408 0.9667271  0.09520251 0.05582838\n",
      " 0.9738071  0.29363406 0.88913184 0.05201242 0.9838521  0.5291817\n",
      " 0.21210812 0.6445948  0.01652415 0.9447022  0.05201242 0.27347812\n",
      " 0.01217063 0.9700869  0.02039132 0.04270578 0.01132676 0.00797145\n",
      " 0.9632667  0.6132546  0.01760441 0.02501406 0.01876311 0.02460657\n",
      " 0.87599814 0.05201242 0.15306222 0.23137484 0.06105006 0.02098469\n",
      " 0.05201242 0.9756177  0.07594976 0.02385364 0.03609586 0.01008544\n",
      " 0.06609312 0.04742766 0.00854731 0.08431567 0.06049239 0.02631722\n",
      " 0.11154423 0.01515578 0.03106942 0.05201242 0.09376087 0.19014162\n",
      " 0.93251973 0.01652415 0.02258355 0.06609312 0.9270567  0.09693238\n",
      " 0.77260244 0.95037323 0.19711892 0.97100306 0.16134167 0.07175529\n",
      " 0.0920386  0.91445714 0.02390428 0.05201242 0.05201242 0.7704669\n",
      " 0.01652415 0.8842139  0.7963977  0.9763248  0.01044433 0.02741131\n",
      " 0.2676869  0.01652415 0.01213187 0.01652415 0.9749508  0.9724992\n",
      " 0.86778057 0.48125005 0.24472345 0.01652415 0.96839696 0.9587984\n",
      " 0.04413215 0.2297047  0.96856064 0.26559353 0.8793002  0.96423507\n",
      " 0.93791044 0.9797586  0.03756216 0.9166429  0.01842143 0.01652415\n",
      " 0.01061733 0.86092687 0.9519361  0.0197968  0.98339844 0.01771036\n",
      " 0.05464746 0.06592734 0.01312202 0.9641851  0.9552502  0.9489095\n",
      " 0.05201242 0.01311972 0.10686314 0.0557309  0.94528675 0.95535356\n",
      " 0.02713181 0.9749508  0.9784671  0.0584032  0.05701961 0.01626955\n",
      " 0.93983054 0.97712815 0.9375139  0.01028073 0.87987274 0.39588213\n",
      " 0.06529725 0.04366426 0.95042706 0.00897339 0.04747988 0.98576987\n",
      " 0.02652873 0.7765041  0.95604366 0.02005398 0.95028675 0.01739191\n",
      " 0.4920172  0.0761686  0.9805894  0.03205124 0.01777235 0.08003447\n",
      " 0.98077536 0.06956483 0.94743323 0.0128664  0.046926   0.02358586\n",
      " 0.2389544  0.04035203 0.8958882  0.9631804  0.83044237 0.981605\n",
      " 0.98064786 0.9191429  0.9466331  0.9734856  0.05201242 0.01823534\n",
      " 0.26644683 0.15536626 0.95184296 0.93441063 0.77260244 0.95972604\n",
      " 0.9719092  0.05562092 0.9365079  0.5231321  0.01649702 0.54963267\n",
      " 0.02863465 0.09537044 0.07644925 0.9210713  0.94041413 0.00910323\n",
      " 0.97455037 0.01017936 0.0171806  0.01431684 0.02071263 0.02711339\n",
      " 0.03614154]\n"
     ]
    }
   ],
   "source": [
    "y_test_prob = classifier.predict_proba(test_data_dash)[:,1]\n",
    "print(y_test_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
